<!DOCTYPE html>
<html lang="ru">
   <head>
        <title>Performance indicators of virtual machine migration</title> 
        <link rel="stylesheet" href="css/style.css" />
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8"/>
        <style>
            p {text-indent: 20px;}
            p {font-size: 20px;}

            h1 {text-align: center;}
            h1 {font-size: 45px;}

            h2 {font-size: 35px;}

            ul {font-size: 20px;}
            ol {font-size: 20px;}

            h3 {font-size: 25px;}

            img 

        </style>
   </head>
   <body>
      <h1>Показатели эффективности миграции виртуальных машин</h1>
      <h2>Оглавление</h2>
      <p>1. Введение <br/> 1.1. Облачные вычисления <br/> 1.2. Преимущества облачных вычислений <br/> 1.3. Живая миграция <br/> 1.3.1. Особенности живой миграции</p>
      <p>2. Актуальность задачи <br/> 2.1. Проблема итеративного перераспределения <br/> 2.2. Существующие алгоритмы балансировки нагрузки <br/> 2.3. Существующие подходы к оценке времени миграции</p>
      <p>3. Оценка времени миграции на основе линейной регрессионной модели 17 <br/> 3.1. Постановка задачи <br/> 3.2. Методология эксперимента <br/> 3.2.1. Описание стенда <br/> 3.2.2. Выбор полезной нагрузки <br/> 3.3. Результаты </p> 
      <p>4. Выводы </p>
      <p>Список литературы</p>
    <center>
      <h2>1. Введение</h2>
    </center>
      <h3>1.1. Облачные вычисления</h3>
      <p>Облачные вычисления — информационно-технологическая
концепция, подразумевающая обеспечение повсеместного и удобного
сетевого доступа по требованию к общему пулу конфигурируемых
вычислительных ресурсов (например, сетям передачи данных, серверам,
устройствам хранения данных, приложениям и сервисам — как вместе, так
и по отдельности), которые могут быть оперативно предоставлены и
освобождены с минимальными эксплуатационными затратами или
обращениями к провайдеру [1].</p>
       <p>Так же национальным институтом стандартов и технологий США
даются определения терминов [1]:</p>
      <ul>
         <li>Самообслуживание по требованию — потребитель
самостоятельно определяет и изменяет вычислительные потребности,
такие как серверное время, скорости доступа и обработки данных, объём
хранимых данных без взаимодействия с представителем поставщика
услуг;</li>
         <li>Универсальный доступ по сети — услуги доступны
потребителям по сети передачи данных вне зависимости от используемого
терминального устройства;</li>
         <li>Объединение ресурсов — поставщик услуг объединяет
ресурсы для обслуживания большого числа потребителей в единый пул
для динамического перераспределения мощностей между потребителями в
условиях постоянного изменения спроса на мощности; при этом
потребители контролируют только основные параметры услуги
(например, объём данных, скорость доступа), но фактическое
распределение ресурсов, предоставляемых потребителю, осуществляет
поставщик (в некоторых случаях потребители всё-таки могут управлять некоторыми физическими параметрами перераспределения, например,
указывать желаемый центр обработки данных из соображений
географической близости);</li>
         <li>Эластичность — услуги могут быть предоставлены,
расширены, сужены в любой момент времени, без дополнительных
издержек на взаимодействие с поставщиком, как правило, в
автоматическом режиме;</li>
         <li>Учёт потребления — поставщик услуг автоматически
исчисляет потреблённые ресурсы на определённом уровне абстракции
(например, объём хранимых данных, пропускная способность, количество
пользователей, количество транзакций), и на основе этих данных
оценивает объём предоставленных потребителям услуг.
Стоит отметить несколько принципов, которые отличают облачные
вычисления [2]</li>
         <li>Предоставление «бесконечных» вычислительных ресурсов по
требованию на случай пиковых нагрузок, тем самым устраняя
необходимость долгосрочного (для пользователя) планирования резервов.</li>
         <li>Отказ от авансовых обязательств для облачных пользователей.
Это позволяет компаниям начать с малого и увеличивать аппаратные
ресурсы только тогда, когда есть увеличение их потребностей.</li>
         <li>Возможность платить за использование вычислительных
ресурсов на краткосрочной основе по мере необходимости (например,
почасовая оплата процессоров или поденная оплата хранилищ) и
освобождать их по мере необходимости, тем самым поощряя
освобождение машин и хранилищ, когда они больше не нужны.</li>
      </ul>
      <h3>1.2. Преимущества облачных вычислений</h3>
      <p>Для того чтобы понять, как возникла задача балансировки ресурсов,
рассмотрим три сценария, в которых облачный подход к вычислениям
(utilitycomputing) предпочтительнее традиционного хостинга [2]. В первом
случае востребованность сервиса сильно меняется со временем. Например,
при традиционном подходе выделение ресурсов дата-центра,
позволяющие справится с редкими (несколько дней в месяц) пиковыми
нагрузками, приводит к неэффективному использованию ресурсов. В
качестве альтернативы облачная модель позволяет организации проводить
почасовую оплату вычислительных мощностей, что потенциально ведёт к
снижению затрат, даже если аренда машины у поставщика облачных услуг
превышает затраты на содержание собственной (почасовые затраты). Во
втором рассматриваемом сценарии неизвестно, какое количество ресурсов
будет необходимо. К примеру, веб-стартап будет нуждаться в технической
поддержке ажиотажного спроса при резком росте популярности, но
потенциально спрос на сервис может упасть, и тогда часть ресурсов
можно будет освободить. Третий сценарий применим к компаниям,
занимающимся ресурсоёмкими вычислениями (например, анализом
крупных массивов данных), которые могут воспользоваться
«ассоциативностью затрат» для ускорения обработки данных. Это
возможно за счёт использования 1000 виртуальных машин в течении часа,
вместо использования одной машины на протяжении 1000 часов.
Таким образом облачные вычисления позволяют заказчикам
распределить затраты на вычислительные ресурсы не равномерно, а в
соответствии с реальной необходимостью этих ресурсов (Michael
Armbrust, 2010) (например, использовав 100 машино-часов сегодня и ни
одного завтра, платить только за 100).</p>
      <p>Ещё одно достижение облачных вычислений – консолидация
ресурсов. Для традиционных дата-центров оценки средней загруженности
6
колеблются между 5% и 20% [3, 4]. Это связано с тем, что пиковые
непродолжительные нагрузки могут быть на порядок выше средней
загруженности и ресурсы, предназначенные для работы при пиковых
нагрузках, простаивают при обычных сценариях. Так как ввод в строй
новых вычислительных мощностей при традиционных подходах может
занимать от нескольких дней до нескольких недель (доставка
оборудования, включение его в инфраструктуру, настройка), то
использование таких подходов приводит либо к низкой эффективности
использования ресурсов, либо к отказам системы при критических
сценариях. Благодаря эластичности облачных систем становится
возможно динамически распределять вычислительные мощности между
сервисами, тем самым потенциально повысив среднюю эффективность
серверов.</p>
      <p>Для того чтобы предоставлять сервис такого рода, поставщики
облаков должны иметь возможность быстро перераспределять нагрузку
между физическими серверами. Для этого уже сейчас используют
технологии виртуализации и живой миграции виртуальных машин.</p>
      <h3>1.3. Живая миграция</h3>
     <center>
      <p>При рассмотрении этапов миграции будет опираться на работу [5]</p>
      <img src="img/cover.jpg"/>
      <p><em>Рисунок 1: Основные этапы миграции.</em></p>
     </center>
      <p>Логические этапы миграции виртуального окружения представлены
на Рисунок 1: Основные этапы миграции. Миграция как процесс
транзакционного взаимодействия двух серверов включает:</p>
      <p><strong>0. Этап пре-миграции.</strong> На этом этапе выбирается принимающий
сервер и проверяется, может ли он гарантировать достаточно ресурсов для
переносимой ВМ.</p>
       <p><strong>1. Этап резервации.</strong> Создаётся запрос на миграцию ВМ с сервера А на
сервер Б. Проходит проверка наличия достаточного количества ресурсов и
резервирование их в соответствии с размерами ВМ.</p>
       <p><strong>2. Этап итеративного предкопирования.</strong> Во время первой итерации
передаются все страницы памяти. На последующих итерациях передаются
только те страницы, которые были загрязнены за время предыдущей
итерации. Этот этап завершается при выполнении какого-либо из условий
остановки, которые будут описаны подробнее ниже.</p>
       <p><strong>3. Этап остановки и копирования.</strong> Выполнение ВМ на сервере А
приостанавливается, сетевой трафик перенаправляется от сервера А на
сервер Б. После этого передаётся состояние виртуального ЦПУ и
оставшиеся некогерентные страницы памяти. После этого этапа на
серверах А и Б виртуальная машина находится в консистентном
приостановленном состоянии.</p>
       <p><strong>4. Этап подтверждения.</strong> Сервер Б уведомляет А об успешном
получении консистентного образа ВМ. Сервер А принимает это
сообщение в качестве подтверждения успешного завершения транзакции
миграции, после чего освобождает ресурсы, принадлежащие
«оригинальной» ВМ.</p>
       <p><strong>5. Этап активации.</strong> Перемещённая ВМ активируется на сервере Б,
происходит выполнение постмиграционных сценариев, которые
подключают устройства в соответствии с конфигурацией и проводят
необходимые манипуляции с сетью.</p> 
       <p>При таком подходе во время миграции хотя бы один из серверов
будет содержать консистентный образ ВМ. Таким образом в случае отказа
какого-либо рода во время миграции ВМ на новый сервер процесс
миграции может быть отменён, а работа ВМ продолжена на локальном
сервере.</p>
       <p>В дальнейшем будем различать два временных промежутка.
Длительность всех пяти этапов будем называть <em> временем миграции, </em> а
суммарную длительность этапов 3 и 4 – <em> временем простоя. </em></p>
       <h3>1.3.1. Особенности живой миграции</h3>
       <p>Во время живой миграции производительность системы страдает от
накладных расходов на поддержание когерентности образа памяти ВМ [5].
Очевидно, что если исключить этап итеративного копирования, вся
миграция пройдёт за время, пропорциональное размеру памяти. К
сожалению, это приведёт к недоступности всех сервисов, которые
предоставляет данная ВМ, на время миграции. В системах с требованием к
доступности сервисов 99,999% времени (т.е. сервис может быть
недоступен ~5 минут в год) применение такого подхода недопустимо
(миграция ВМ с 8ГБ памяти будет приводить к простою сервисов порядка
2-х минут).</p>
       <p>Более привлекательной становится альтернатива с использованием
итеративного копирования памяти, во время которой образ памяти
передаётся в то время, пока ВМ (а следовательно, и все предоставляемые
ей сервисы) продолжает работать. Тем не менее, недостаток этого подхода
в том, что за время передачи памяти некоторые страницы могут быть
повторно модифицированы, как следствие, их нужно будет передать снова
(в дальнейшем такие страницы будем называть <em>грязными</em>). Для
большинства нагрузок существует небольшое множество часто
изменяемых страниц, попытки передачи страниц из этого множества до
остановки ВМ не принесут пользы. Фундаментальным вопросом миграции
с итеративной передачей памяти является следующий вопрос: «Как
определить, что нужно завершить этап передачи данных из-за того, что
было потрачено слишком много времени и ресурсов?» Если ВМ за время
миграции не модифицирует ни одной страницы памяти, то достаточно будет одной итерации предкопирования. В то же время, если ВМ будет
загрязнять память быстрее, чем она будет переносится, то очевидно, что
вся идея предкопирования окажется бессмысленной, и нужно перейти к
этапу «остановки и копирования».</p>
           <p>Наиболее вероятные нагрузки лежат где-то между этими двумя
крайностями: определённое (возможно большое) множество страниц будет
изменяться редко или практически никогда, а следовательно, это лучший
кандидат для предкопирования, в тоже время оставшееся множество
страниц (<em>рабочий набор</em>) следует передавать на стадии остановки и
копирования.</p>
           <p>В рассматриваемых платформах используются следующие условия
остановки (проверяются после выполнения итерации):</p>
       <ol>
           <li>Было передано страниц больше чем половина от объёма
предыдущей итерации.</li>
           <li>Переданных страниц меньше ста.</li>
       </ol>
       <p>Таким образом живая миграция является технологией, которая
позволяет обеспечивать быстрое и прозрачное для пользовательских
систем перераспределение, но для балансировки нагрузки на серверы при
пере- или недозагружености необходимо принять решение о том, с каких
серверов, какие ВМ и куда нужно перенести.</p>
       <h3>2. Актуальность задачи</h3>
       <p>Широкое распространение облачных вычислений [6, 7] предъявляет
все большие требования к качеству предоставляемых услуг. Как
отмечалось ранее [1, 2] облачные вычисления подразумевают эластичное
предоставление ресурсов. Для обеспечения эластичности необходим
эффективный менеджмент ресурсов, который невозможен без оценки
времени миграции. Дело в том, что когда менеджер ресурсов
обнаруживает ситуацию нехватки ресурсов на одном из серверов, он
должен:</p>
       <ol>
           <li>Оценить как долго нехватка продлится</li>
           <li>Выбрать потенциального “эмигранта"</li>
           <li>Соотнести это время со временем миграции “эмигранта”</li>
       </ol>
       <p>Такая последовательность исключает лишние миграции, могущие
снизить общую производительность системы. Таким образом, оценка
времени миграции является подзадачей важнейшей современной
проблемы распределения ресурсов в облаке.</p>
       <h3>2.1. Проблема итеративного перераспределения</h3>
       <p>Проблема размещения виртуальных машин начинается с
определения того, как машины уже были распределены на физических
хостах, и любой алгоритм, который решает данную проблему, должен
генерировать решения в соответствии с уже данным распределением.
Такие решения - списки последовательных и параллельных миграций,
которые должны быть выполнены для перевода дата-центра из текущего
состояния в желаемое. Все исполняемые живые миграции, все
промежуточные состояния должны удовлетворять по крайней мере
подмножеству поставленных ограничений; некоторые ограничения, в то
же время, могут быть ослаблены на время данного перехода. Например,
если виртуализационное программное обеспечение поддерживает
oversubscribing оперативной памяти, может быть позволено превысить
физическую память хоста на время миграций, даже если политика датацентра в общем случае этого не позволяет. Также бывает необходимо иметь промежуточное состояние хуже, чем изначальное, для достижения
лучшего конечного состояния. Внешние условия также могут приводить к
ослаблению ограничений: например, пожар или неполадки с системой
охлаждения делают допустимым определенный уровень перегрузки, в
общем случае недозволенный. Виртуальные машины и физические хосты
могут быть добавлены или убраны из дата-центра, загрузки по ресурсам
могут различаться, так что качество любого размещения виртуальных
машин может ухудшаться со временем и должно оцениваться заново
каждый раз, когда подобное происходит. Задачу оценки изначальной
конфигурации дата-центра и генерации списка живых миграций,
улучшающих качество размещения виртуальных машин, называют
проблемой итеративного перераспределения [8].</p>
       <h3>2.2. Существующие алгоритмы балансировки нагрузки</h3>
     <center>
       <p>Для решения задачи итеративного перераспределения алгоритмы
балансировки нагрузки в общем случае имеют логическое строение,
представленное на Рисунок 2.</p>
       <img src="img/cover2.jpg"/>
       <p><em>Рисунок 2: Обобщённая архитектура менеджера ресурсов.</em></p>
     </center>
       <p>Алгоритм можно условно разделить на следующие этапы:</p>
       <ol>
           <li>Подготовительная фаза. Менеджер на этом этапе собирает на
работающих серверах информацию об используемых ресурсах. Способ
получения, равно как и объём собираемых данных, зависит от
конкретной реализации. На основе предварительной обработки
принимается решение о запуске перераспределения ресурсов в случае
нарушения предварительно заданных условий.</li>
           <li>Фаза планирования миграции. Во время этого этапа менеджер
непосредственно обрабатывает полученную информацию и на основании
конкретного алгоритма формирует план миграции, который должен
устранить или минимизировать возникшие нарушения условий. Обычно
такой план состоит из троек «физическая машина отправитель,
виртуальная машина, физическая машина получатель».</li>
           <li>Фаза выполнения миграции. На этом этапе происходит выполнение
плана миграции менеджером виртуальных машин. Особенности этой
фазы могут зависеть от используемой технологии виртуализации. <br/> В исследовании [9] были рассмотрены ряд менеджеров ресурсов,
использующих различные алгоритмы.</li>
       </ol>
       <ul>
           <li>DMA [10] при решении задачи руководствуется
метрикой «затраты на миграцию», которая является линейной
функцией от утилизации процессорного времени и оперативной
памяти. Данная метрика предполагается одинаковой для всех ВМ в
дата-центре, но в работе [10] отсутствует описание методики
получения коэффициентов</li>
           <li>Алгоритм менеджера Sandpiper [11] при упорядочивании
ВМ использует функцию от утилизации процессорного времени,
оперативной памяти и сетевого ресурса:</li>
       </ul>
        <img src="img/cover3.jpg"/>
        <p>Все дальнейшие решения данный алгоритм принимает на основании
значения данного параметра для каждой ВМ.</p>
        <ul>
            <li>Алгоритм, представленный Андреолини и Касолари [12],
основывается при принятии решения на поведенческом тренде
использования процессорного времени ВМ.</li>
        </ul>
        <p>Таким образом ни один из опубликованных алгоритмов не
учитывает время миграции каждой ВМ в явном виде, а некоторые [11, 12]
основаны на предположении что время миграции одинаково и
пренебрежимо мало. Использование информации о времени миграции
потенциально может улучшить результат перераспределения нагрузки в
при подкритических ситуациях. Так же в одном из алгоритмов [12] в
качестве горизонта предсказания было предложено использовать 15
минут. С увеличением горизонта предсказания точность самого предсказания падает, поэтому естественно стремится к его уменьшению.
Таким образом возникает проблема оценки времени миграции.</p>
        <h3>2.3. Существующие подходы к оценке времени миграции</h3>
        <p>На данный момент существует несколько моделей предсказания
времени миграции[5, 10, 15]. Одна из таких моделей, предложена Сериф
Акоуш и Рипдуман Сохан [14]</p>
        <p>Данная модель делает вывод о времени миграции на основании
скорости загрязнения страниц, пропускной способности канала, размера
передаваемой виртуальной машины и информации о накладных расходах
в используемом алгоритме миграции. По своей сути предложенная в
статье модель - симуляция миграции, в ходе которой на основе
информации о скорости генерируется битовая карта грязных страниц.
Данный подход позволил получить высокую точность предсказания
времени миграции (ошибка колеблется от 1,5% до 9,5% в зависимости от
нагрузки). В то же время получение такого параметра как скорость
загрязнения страниц связано с большими накладными расходами и тесной
интеграцией с виртуализационной платформой.</p>
        <p>Так же были попытки оценки времени миграции с использованием
статистического аппарата, предпринятые Ву и Чао [15]. Были получены
зависимости от суммарного использования процессора на сервере, при
разной интенсивности операций с памятью. Но в итоговой модели нет
зависимости от нагрузки на память, а характер зависимости времени
миграции от загруженности процессора меняется значительно.</p>
        <p>В своей работе, Гунджан Ханна с соавторами [10] в рамках метрики
оценки загруженности машины рассматривают величину очень близкую к
времени миграции, а именно затраты на миграцию. В своей оценке авторы
делают предположение о линейной зависимости использования оперативной памяти и процессора. К сожалению методика получения
коэффициентов для этой модели не приводится.</p>
     <center>
       <h2>3. Оценка времени миграции на основе линейной
регрессионной модели</h2>
     </center>
       <h3>3.1. Постановка задачи</h3>
       <p>Существующие решения задачи автоматической балансировки
нагрузки в облаке на данный момент не учитывают время миграции
гостевых ВМ [10, 11, 12, 13], что может привести не только к получению
неоптимального решения задачи, но и к серьёзному ухудшению качества
работы системы (из-за резкого роста накладных расходов на серверах,
вовлечённых в процесс миграции). Таким образом проблема оценки
времени миграции актуальна</p>
       <p>В существующих работах [14] было показано, что наиболее
значимыми параметрами при оценке времени миграции является ширина
канала передачи данных и скорость загрязнения страниц памяти. Со
скоростью передачи данных всё относительно легко, т.к. она может быть
измерена быстро и с минимальными накладными расходами, более того,
большинство поставщиков облачных платформ советуют использовать для
миграции отдельную сеть. Измерение второго параметра сопряжено с
высокими накладными расходами (выход в гипервизор и перевод всей
памяти гостевой ВМ в режим «только чтение»), более того, этот параметр
в реальных сценариях быстро меняется, и для получения
удовлетворительных данных необходимо усреднение по времени,
сравнимым с временем миграции. Следует отметить, что в сентябре 2014
года фирма Intel® выпустила новое поколение процессоров Xeon® E5-
2600 v3, в которых появилась возможность проверки A/Dbit’a для гостевой
памяти[16], что снизит накладные расходы на измерения скорости загрязнения памяти, но, тем не менее, проблема усреднения остаётся
открытой, равно как проблема в целом для аппаратных платформ
предыдущих поколений.</p> 
       <p>В данной работе предлагается способ нахождения зависимости
времени миграции от доступных метрик эмпирическим путём, используя
статистические модели.</p>
       <h3>3.2. Методология эксперимента</h3>
       <p>В качестве объектов исследования были выбраны платформа:
ParallelsCloudServer. Время миграции было отождествлено с временем
выполнения команд для миграции ВМ. В соответствие каждой миграции
были поставлены вывод команды sar с опциями «выводить всё». Таким
образом мы получаем максимально полную, быстро доступную статистику
по используемым системой ресурсам, не создавая больших накладных
расходов. Целью эксперимента был поиск коррелирующих со временем
миграции признаков, входящих в системные статистики, а также оценка
характера зависимости от таких признаков. Кроме внутренних факторов
(степень и характер загруженности гостевой системы) в модель был
включён внешний фактор: скорость передачи данными между серверами
А и Б.</p>
       <p>В рамках данной работы была выбрана линейная регрессионная
модель для поиска влияющих на время миграции параметров и для
количественного описания найденного влияния.</p>
       <h3>3.2.1. Описание стенда</h3>
     <center>
       <p>Конфигурация стенда для проведения экспериментов приведена на
Рисунок 3.</p>
       <img src="img/cover4.jpg"/>
       <p><em>Рисунок 3: Схема экспериментальной установки.</em></p>
     </center>
       <p>В стенде были использованы три сервера - каждый с процессором
Intel® Core™ i7-920, 4GBDDR3 RAM, сетевой картой 1 GigabitEthernet и
сетевым коммутатором D-Link. Дополнительно в виртуализационных
серверах были установлены сетевые карты Infiniband.</p>
      <p>На серверах А и Б была установлена платформа ParallelsCloudServer
6.0.8, сетевое хранилище основано на дистрибутиве CentOS 7.1.1503 и
пакете linux-nfs.</p>
       <h3>3.2.2. Выбор полезной нагрузки</h3>
       <p>Предлагаемая модель предсказания времени миграции во многом
зависит от разнообразия собранных статистических данных. В связи с
этим были использованы девять типов нагрузок: синтетическая нагрузка,
загрязняющая память с постоянной скоростью, и восемь нагрузок из
пакета SPECjvm. Использованная синтетическая нагрузка сходна с
таковой из предыдущих исследований [14]. Особенность этой
синтетической нагрузки заключается в том, что она моделирует
практически не встречающуюся в жизни ситуацию, когда процесс с
постоянной скоростью перезаписывает один и тот же объём памяти. При
таком сценарии происходит довольно мало итераций миграции, а время простоя определяется объёмом памяти, с которым работает нагрузка. <br/> В ходе проведённых исследований было необходимо
удостовериться, что выбранная модель даёт удовлетворительные
результаты при работе с распространёнными нагрузками. В качестве
таковых были выбраны нагрузки из системы оценки SPECjvm. Данный
набор нагрузок состоит из приложений, решающих распространённые
задачи, в то же время интенсивно используется не только процессорное
время, но и оперативная память, что приводит к различному времени
миграции виртуальной среды.</p>
       <h3>3.3. Результаты</h3>
       <p>Было проведено более тысячи экспериментов по миграции ВМ при
различных нагрузках. На Рисунок 4 и Рисунке Рисунок 5 приведены
графики распределения времени миграции от использования процессора и
оперативной памяти, так как именно эти параметры оказывают
статистически значимое влияние.</p>
     <center>
        <img src="img/cover5.jpg"/>
        <p><em>Рисунок 4: График зависимости времени миграции от использования
процессора</em></p>
        <img src="img/cover6.jpg"/>
        <p><em>Рисунок 5: Зависимость времени миграции от использованной
оперативной памяти.</em></p>
     </center>
        <p>В результате итеративного добавления статистически значимых
параметров к проверяемой модели был получен следующий вид
зависимости: <br/> Время миграции ~ 1 + 𝐶𝑃𝑈.𝑎𝑙𝑙..𝑖𝑑𝑙𝑒 + 𝑘𝑏𝑐𝑜𝑚𝑚𝑖</p>
        <p>В данной формуле фиксируется зависимость от параметров:</p>
        <ul>
            <li>CPU.all..idle – суммарный процент времени, во время которого
процессоры простаивали.</li>
            <li>Kbcommit – ориентировочное количество памяти в килобайтах
необходимое для текущей работы с точки зрения утилиты sar.
Построение линейной регрессионной модели дало результаты,
приведённые в Таблица 1.</li>
        </ul>
     <center>
        <img src="img/cover7.jpg"/>
        <p><em>Таблица 1: Результаты построения регрессионной модели.</em></p>
     </center>
        <p>Построенная модель обладает коэффициентом детерминации 0,5375,
что позволяет признать её приемлемой.</p>
     <center>
        <h2>4. Выводы</h2>
        <p>В работе была показана состоятельность оценки времени миграции как линейная регрессионная модель от параметров характеризующих суммарный простой процессоров и необходимый для работы объём памяти. Кроме того, была сформулирована проблема оценки времени миграции и проведен детальный анализ существующих решений. Хотелось бы подчеркнуть, что при всей значимости данной оценки многие из академических исследований распределения ресурсов склонны ее игнорировать, что делает самую красивую модель неприменимой на практике. Достоинством этого подхода являются низкие накладные расходы в сравнении с предложенным в предыдущих работах [14]. Данная модель может применяться в случаях, когда важны скорость получения оценки и/или минимизация влияния на работоспособность виртуального окружения.</p>
     </center>
        <h2>Список литературы</h2>
        <ol>
            <li>Mell P., Grance T. The NIST definition of cloud computing. – 2011</li>
            <li>Armbrust M. et al. A view of cloud computing //Communications of the ACM. – 2010. – Т. 53. – №. 4. – С. 50-58.</li>
            <li>Rangan, K. The Cloud Wars: $100+ Billion at Stake. Tech. Rep., Merrill Lynch, May 2008</li>
            <li>Siegele L. Let it rise: A special report on corporate IT. – Economist Newspaper, 2008</li>
            <li>Clark C. et al. Live migration of virtual machines //Proceedings of the 2nd conference on Symposium on Networked Systems Design & Implementation-Volume 2. – USENIX Association, 2005. – С. 273-286</li>
            <li>Site H. P. W. Introducing Integrity Virtual Machines white paper.</li>
            <li>Site V. M. W. W. Server Consolidation and Containment Solutions Brief.</li>
            <li>Hyser C. et al. Autonomic virtual machine placement in the data center. – 2008.</li>
            <li>Abdul-Rahman O., Munetomo M., Akama K. Live migration-based resource managers for virtualized environments: a survey //Proc. of the 1st International Conference on Cloud Computing, GRIDs, and Virtualization (CLOUD COMPUTING 2010). – 2010. – С. 32-40.</li>
            <li>Khanna G. et al. Application performance management in virtualized server environments //Network Operations and Management Symposium, 2006. NOMS 2006. 10th IEEE/IFIP. – IEEE, 2006. – С. 373-381.</li>
            <li>Wood T. et al. Black-box and Gray-box Strategies for Virtual Machine Migration //NSDI. – 2007. – Т. 7. – С. 17-17.</li>
            <li>Andreolini M. et al. Dynamic load management of virtual machines in cloud architectures //Cloud Computing. – Springer Berlin Heidelberg, 2010. – С. 201-214.</li>
            <li>Dhiman G., Marchetti G., Rosing T. vGreen: a system for energy efficient computing in virtualized environments //Proceedings of the 2009 ACM/IEEE international symposium on Low power electronics and design. – ACM, 2009. – С. 243-248.</li>
            <li>Akoush S. et al. Predicting the performance of virtual machine migration //Modeling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), 2010 IEEE International Symposium on. – IEEE, 2010. – С. 37-46.</li>
            <li>Wu Y., Zhao M. Performance modeling of virtual machine live migration //Cloud Computing (CLOUD), 2011 IEEE International Conference on. – IEEE, 2011. – С. 492-499.</li>
            <li>Site Intel. Are you ready to innovate - four new Virtualization +technologies on the latest Intel® Xeon!</li>
        </ol>
     </body>
</html>
